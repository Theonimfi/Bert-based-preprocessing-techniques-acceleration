{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Bert_based_evaluation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Theonimfi/Bert-based-preprocessing-techniques-acceleration/blob/main/src/Bert_based_evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQK0fCYfAPkG"
      },
      "source": [
        "## Importing the necessary modules:\n",
        "1. The tensorflow(1.15.0) versions need to be correctly compatible with the bert-tensorflow version of 1.0.1. The corresponding bert modules like 'tokenization' performing tokenization on the input sentence.\n",
        "2. We use the nltk toolkit for our pre-processing tasks.\n",
        "2. Libraries like pandas,numpy are used for basic operations. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmYuMREyU7uV"
      },
      "source": [
        "pip install tensorflow==1.15.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRr58oojAMDd"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wx_lA33TUq8"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from datetime import datetime\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "\n",
        "print(\"tensorflow version : \", tf.__version__)\n",
        "print(\"tensorflow_hub version : \", hub.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifLEoti0TeIN"
      },
      "source": [
        "!pip install bert-tensorflow==1.0.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSWxsd7ETgD0"
      },
      "source": [
        "#Importing BERT modules\n",
        "import bert\n",
        "from bert import run_classifier\n",
        "from bert import optimization\n",
        "from bert import tokenization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LodL6gFvVM1J"
      },
      "source": [
        "# Load the dataset.\n",
        "!pip install numpy\n",
        "!pip install pandas\n",
        "!pip install nltk\n",
        "!pip install -q -U tensorflow-text\n",
        "!pip install -q tf-models-official\n",
        "!pip install contractions\n",
        "!pip install textblob\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3NRSCuXBv_r"
      },
      "source": [
        "# Loading the Datasets :\n",
        "We use a total of 4 datasets to test our preprocessing techniques by fine-tuning the BERT model.\n",
        " We set our datasets as the following :\n",
        " 1. DATASET_1: The Twitter Airline Dataset https://data.world/crowdflower/airline-twitter-sentiment\n",
        " 2. DATASET_2: Amazon Consumer Reviews Dataset https://www.kaggle.com/datafiniti/consumer-reviews-of-amazon-products\n",
        " 3. DATASET_3: Amazon Automotive Reviews Dataset https://jmcauley.ucsd.edu/data/amazon/\n",
        " 4. DATASET_4 :Twitter Sentiment Dataset for Self-driving cars https://data.world/crowdflower/sentiment-self-driving-cars\n",
        "\n",
        "We set the corresponding dataset we test on to \"True\" and the others to \"False\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqAgCFteDlDR"
      },
      "source": [
        "DATASET_1 = True\n",
        "DATASET_2 = False\n",
        "DATASET_3 = False\n",
        "DATASET_4 = False\n",
        "\n",
        "# AIRLINE TWITTER DATASET\n",
        "if DATASET_1:\n",
        "  # https://www.codegrepper.com/code-examples/python/how+to+read+csv+file+from+google+drive+on+google+colab+\n",
        "  #Derive the id from the google drive shareable link.\n",
        "  #For the file at hand the link is as below\n",
        "  URL = 'https://drive.google.com/file/d/10j1dwpPCk7XAHCTX7gczX2woXzMZ5Dva/view?usp=sharing'\n",
        "  path = 'https://drive.google.com/uc?export=download&id='+URL.split('/')[-2]\n",
        "  #df = pd.read_pickle(path)\n",
        "  df = pd.read_csv(path)\n",
        "  # df = df.head(100)\n",
        "  df = df[['text','airline_sentiment']]\n",
        "  DATA_COLUMN = 'text'\n",
        "  LABEL_COLUMN = 'airline_sentiment'\n",
        "  cleanup_nums = {\"airline_sentiment\": {\"positive\": 1, \"negative\": 0,\"neutral\":2}}\n",
        "  df = df.replace(cleanup_nums)\n",
        "  # The list containing all the classes (train['SECTION'].unique())\n",
        "  label_list = [0, 1, 2]\n",
        "  dataset_name = 'D1_'\n",
        "\n",
        "# DATAFININITI AMAZON CONSUMER REVIEWS\n",
        "if DATASET_2:\n",
        "  URL = 'https://drive.google.com/file/d/1EbWy4GSV_Ano6OlOiXZaNuqmJrnc95p5/view?usp=sharing'\n",
        "  path = 'https://drive.google.com/uc?export=download&id='+URL.split('/')[-2]\n",
        "  df = pd.read_csv(path)\n",
        "  df = df[['reviews.rating','reviews.text']]\n",
        "  DATA_COLUMN = 'reviews.text'\n",
        "  LABEL_COLUMN = 'reviews.rating'\n",
        "  dataset_name = 'D2_'\n",
        "  # The list containing all the classes (train['SECTION'].unique())\n",
        "  label_list = [1, 2, 3, 4, 5]\n",
        "  \n",
        "# AMAZON AUTOMOTIVE\n",
        "if DATASET_3:\n",
        "  URL = 'https://drive.google.com/file/d/1-ST5Wffhx9ky56Qh7KEkbfhMd9MRY97d/view?usp=sharing'\n",
        "  path = 'https://drive.google.com/uc?export=download&id='+URL.split('/')[-2]\n",
        "  df = pd.read_json(path, lines=True)\n",
        "  df = df[['reviewText','overall']]\n",
        "  DATA_COLUMN = 'reviewText'\n",
        "  LABEL_COLUMN = 'overall'\n",
        "  dataset_name = 'D3_'\n",
        "  # The list containing all the classes (train['SECTION'].unique())\n",
        "  label_list = [1, 2, 3, 4, 5]\n",
        "\n",
        "# TWEET SENTIMENT OF SELF-DRIVING CARS\n",
        "if DATASET_4:\n",
        "  #Derive the id from the google drive shareable link.\n",
        "  #For the file at hand the link is as below\n",
        "  URL = 'https://drive.google.com/file/d/1XeIrsFJkOnAaly_YxyOQ8pj18b_yilTA/view?usp=sharing'\n",
        "  path = 'https://drive.google.com/uc?export=download&id='+URL.split('/')[-2]\n",
        "  #df = pd.read_pickle(path)\n",
        "  df = pd.read_csv(path,encoding = 'unicode_escape')\n",
        "  # df = df.head(100)\n",
        "  print(df)\n",
        "  df = df[['text','sentiment']]\n",
        "  DATA_COLUMN = 'text'\n",
        "  LABEL_COLUMN = 'sentiment'\n",
        "  cleanup_nums = {\"sentiment\": {\"1\":1,\"2\":2,\"3\":3,\"4\":4,\"5\":5,\"not_relevant\": 0}}\n",
        "  df = df.replace(cleanup_nums)\n",
        "  # The list containing all the classes (train['SECTION'].unique())\n",
        "  label_list = [0, 1, 2, 3, 4, 5]\n",
        "  dataset_name = 'D4_'\n",
        "\n",
        "\n",
        "df\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEykedO4DmdC"
      },
      "source": [
        "# Data Pre-processing techniques :\n",
        "\n",
        "We individually evaluate the preprocessing techniques and do not combine any of them in our testing.The results are correspondingly compared without any techniques too. In case of no preprocessing techniques being applied, we set all of them to \"False\". "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PS1auv_AD7dg"
      },
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk import word_tokenize\n",
        "import string\n",
        "import re\n",
        "from datetime import datetime\n",
        "from textblob import TextBlob\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import word_tokenize, pos_tag\n",
        "import contractions\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "LOWERCASE = False\n",
        "REMOVE_NUMBERS = False\n",
        "REMOVE_HASHTAGS = False\n",
        "REMOVE_MENTIONS = False\n",
        "REMOVE_PUNCTUATIONS = False\n",
        "STEMMING = False\n",
        "\n",
        "REMOVE_URLS = False\n",
        "REPLACE_REPEAT_PUNCTUATION = False # repeat punctuation show intense emotion show we replace the punctuation with a word to capture the intensity of the text\n",
        "HANDLE_CAPITALIZED_WORDS = False # capitalize words show intense emotion show we add a word before each capitalized word to capture the intensity of the text\n",
        "EMOJIS = False # replace emojis with a word which indicates whether it is a positive or negative emoji (eg :) -> POS_EMOJI, :(( -> NEG_EMOJI)\n",
        "\n",
        "# choose which tech we use\n",
        "STOP_WORDS_REMOVAL = False\n",
        "REPEATED_CHARACTERS_REMOVAL = False\n",
        "LEMMATIZATION = False\n",
        "EXTRA_SPACE_REMOVAL = False\n",
        "CONTRACTED_WORDS_EXPANDING = False\n",
        "print(f'Beginning Pre-processing!')\n",
        "current_time = datetime.now()\n",
        "\n",
        "\n",
        "\n",
        "porter = PorterStemmer()\n",
        "prepr = dataset_name + 'None'\n",
        "\n",
        "df['processed_text'] = df[DATA_COLUMN].fillna('').apply(str)\n",
        "if REPLACE_REPEAT_PUNCTUATION:\n",
        "  prepr = dataset_name + 'replace_repeat_punctuation'\n",
        "  df['processed_text'] = df['processed_text'].apply(lambda x: re.sub('\\!\\!+', 'multiExclamationMark ', x))\n",
        "  df['processed_text'] = df['processed_text'].apply(lambda x: re.sub('\\?\\?+', 'multiQuestionMark ', x))\n",
        "\n",
        "\n",
        "\n",
        "if LOWERCASE:\n",
        "  prepr = dataset_name +  'lowercase'\n",
        "  df['processed_text'] = df['processed_text'].apply(lambda x: x.lower())\n",
        "\n",
        "if REMOVE_MENTIONS:\n",
        "  prepr = dataset_name + 'rmv_mentions'\n",
        "  df['processed_text'] = df['processed_text'].apply(lambda x: re.sub(\"@[A-Za-z0-9_]+\",\"\", x))\n",
        "\n",
        "if REMOVE_HASHTAGS:\n",
        "  prepr = dataset_name +  'rmv_hashtags'\n",
        "  df['processed_text'] = df['processed_text'].apply(lambda x: re.sub(\"#[A-Za-z0-9_]+\",\"\", x))\n",
        "\n",
        "if REMOVE_URLS:\n",
        "  prepr = dataset_name +  'rmv_urls'\n",
        "  df['processed_text'] = df['processed_text'].apply(lambda x: re.sub(r'http\\S+', '', x))\n",
        "\n",
        "if REMOVE_NUMBERS:\n",
        "  prepr = dataset_name +  'rmv_numbers'\n",
        "  df['processed_text'] = df['processed_text'].apply(lambda x: re.sub(r'[0-9]+', '', x))\n",
        "\n",
        "if REMOVE_PUNCTUATIONS:\n",
        "  prepr = dataset_name + 'rmv_punctuations'\n",
        "  df['processed_text'] = df['processed_text'].apply(lambda x: \"\".join([char for char in x if char not in string.punctuation]))\n",
        "\n",
        "if HANDLE_CAPITALIZED_WORDS:\n",
        "  prepr = dataset_name + 'habdle_capitalized_words'\n",
        "  df['processed_text'] = df['processed_text'].apply(lambda x: \" \".join([\"ALL_CAPS \" + word if (word.isupper() and len(word) > 2) else word for word in word_tokenize(x)]))\n",
        "\n",
        "# if SPELLING_CORRECTION:\n",
        "#   prepr = dataset_name + 'spelling correction'\n",
        "#   df['processed_text'] = df['processed_text'].apply(lambda x: \" \".join([str(TextBlob(word).correct()) for word in word_tokenize(x)]))\n",
        "\n",
        "if EMOJIS:\n",
        "  prepr = dataset_name +  'emojis'\n",
        "\n",
        "  def find_emojis(text, neg_emoticons, pos_emoticons, neg_emojis, pos_emojis):\n",
        "      words = word_tokenize(text)\n",
        "      for i, _ in enumerate(words):\n",
        "        if words[i] in neg_emojis or words[i] in neg_emoticons:\n",
        "          words[i] = \"NEG_EMOJI\"\n",
        "        elif words[i] in pos_emojis or words[i] in pos_emoticons:\n",
        "          words[i] = \"POS_EMOJI\"\n",
        "      \n",
        "      return \" \".join(words)\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # files from here: https://github.com/modarwish1/sentimentr\n",
        "  URL = 'https://drive.google.com/file/d/1IBTu3IgYECIl1SOdbPr4o8LUM1_avAas/view?usp=sharing'\n",
        "  path = 'https://drive.google.com/uc?export=download&id='+URL.split('/')[-2]\n",
        "  neg_emoticons = pd.read_csv(path, sep=\"\\t\", header=None, usecols=[0])\n",
        "  neg_emoticons.columns = [\"emoticon\"]\n",
        "  neg_emoticons = neg_emoticons[\"emoticon\"].to_list()\n",
        "  # print(neg_emoticons)\n",
        "\n",
        "  URL = 'https://drive.google.com/file/d/148G4WZablFzVQSmEak6DqABarn4EY2do/view?usp=sharing'\n",
        "  path = 'https://drive.google.com/uc?export=download&id='+URL.split('/')[-2]\n",
        "  pos_emoticons = pd.read_csv(path, sep=\"\\t\", header=None, usecols=[0])\n",
        "  pos_emoticons.columns = [\"emoticon\"]\n",
        "  pos_emoticons = pos_emoticons[\"emoticon\"].to_list()\n",
        "  # print(pos_emoticons)\n",
        "\n",
        "  URL = 'https://drive.google.com/file/d/1FTKcH1ackudDaeDi-ZveA9eIUYkJhsRK/view?usp=sharing'\n",
        "  path = 'https://drive.google.com/uc?export=download&id='+URL.split('/')[-2]\n",
        "  neg_emojis = pd.read_csv(path, sep=\"\\t\", header=None, usecols=[0])\n",
        "  neg_emojis.columns = [\"emoticon\"]\n",
        "  neg_emojis = neg_emojis[\"emoticon\"].to_list()\n",
        "  # print(neg_emojis)\n",
        "\n",
        "  URL = 'https://drive.google.com/file/d/1QOLDjOjYtkCBWsGwjfJ4bBGGsDqfg3Ck/view?usp=sharing'\n",
        "  path = 'https://drive.google.com/uc?export=download&id='+URL.split('/')[-2]\n",
        "  pos_emojis = pd.read_csv(path, sep=\"\\t\", header=None, usecols=[0])\n",
        "  pos_emojis.columns = [\"emoticon\"]\n",
        "  pos_emojis = pos_emojis[\"emoticon\"].to_list()\n",
        "  # print(pos_emojis)\n",
        "  df['processed_text'] = df['processed_text'].apply(lambda x: find_emojis(x, neg_emoticons, pos_emoticons, neg_emojis, pos_emojis))\n",
        "\n",
        "if STEMMING:\n",
        "  prepr = dataset_name + 'stemming'\n",
        "  df['processed_text'] = df['processed_text'].apply(lambda x: \" \".join([porter.stem(word) for word in word_tokenize(x)]))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "column_0 = 'processed_text'\n",
        "column_1 = LABEL_COLUMN\n",
        "text_column = df[column_0]\n",
        "\n",
        "# some prepard class and method\n",
        "class RepeatReplacer(object):\n",
        "  def __init__(self):\n",
        "    self.regex = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
        "    self.repl = r'\\1\\2\\3'\n",
        "  \n",
        "  def replace(self, word):\n",
        "    if wordnet.synsets(word):\n",
        "      return word\n",
        "    loop_res = self.regex.sub(self.repl, word)\n",
        "    if(word == loop_res):\n",
        "      return loop_res\n",
        "    else:\n",
        "      return self.replace(loop_res)\n",
        "def get_wordnet_pos(tag):\n",
        "    if tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "\n",
        "\n",
        "# start processing\n",
        "if STOP_WORDS_REMOVAL:\n",
        "  prepr = dataset_name +  'stop_words_removal'\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  # processing the text in loop\n",
        "  for t in text_column:\n",
        "    new_t = ' '.join([word for word in t.lower().split(' ') if word not in stop_words])\n",
        "    text_column.replace({t:new_t}, inplace=True)\n",
        "if REPEATED_CHARACTERS_REMOVAL:\n",
        "  prepr = dataset_name + 'repeated_characters_removal'\n",
        "  replacer = RepeatReplacer()\n",
        "  # processing the text in loop\n",
        "  for t in text_column:\n",
        "    word_list = []\n",
        "    for word in t.lower().split(' '):\n",
        "      word_list.append(replacer.replace(word))\n",
        "    new_t = ' '.join(word_list)\n",
        "    text_column.replace({t:new_t}, inplace=True)\n",
        "if LEMMATIZATION:\n",
        "  prepr = dataset_name +  'lemmatization'\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  # processing the text in loop\n",
        "  for t in text_column:\n",
        "    tokens = word_tokenize(t)\n",
        "    tagged_t = pos_tag(tokens)\n",
        "    lemmas_t = []\n",
        "    for tag in tagged_t:\n",
        "      wordnet_pos = get_wordnet_pos(tag[1]) or wordnet.NOUN\n",
        "      lemmas_t.append(lemmatizer.lemmatize(tag[0], pos=wordnet_pos)) \n",
        "    new_t = ' '.join(lemmas_t)\n",
        "    text_column.replace({t:new_t}, inplace=True)\n",
        "if EXTRA_SPACE_REMOVAL:\n",
        "  prepr = dataset_name +  'extra_space_removal'\n",
        "  # processing the text in loop\n",
        "  for t in text_column:\n",
        "    new_t = re.sub(' +', ' ', t)\n",
        "    text_column.replace({t:new_t}, inplace=True)\n",
        "if CONTRACTED_WORDS_EXPANDING:\n",
        "  prepr = dataset_name +  'contracted_words_expanding'\n",
        "# processing the text in loop\n",
        "  for t in text_column:\n",
        "    word_list = []\n",
        "    for word in t.lower().split(' '):\n",
        "      word_list.append(contractions.fix(word))\n",
        "    new_t = ' '.join(word_list)\n",
        "    text_column.replace({t:new_t}, inplace=True)\n",
        "\n",
        "\n",
        "df[DATA_COLUMN] = df['processed_text']\n",
        "\n",
        "print(\"Pre-processing took time \", datetime.now() - current_time)\n",
        "\n",
        "# with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
        "print(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bE5J10Xeqew"
      },
      "source": [
        "df[DATA_COLUMN].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nZoFkjmVkHq"
      },
      "source": [
        "filtered_columns=[DATA_COLUMN, LABEL_COLUMN]\n",
        "df= df.reindex(columns=filtered_columns)\n",
        "df.head()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkdSti-GVU_y"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train, val = train_test_split(df, test_size=0.2)\n",
        "train.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ah9Ni0YwGFIH"
      },
      "source": [
        "# Setting up the input to BERT\n",
        "The data in our dataframe needs to be transformed intoa format BERT understands. \n",
        "We create InputExample's using the constructor provided in the BERT library.\n",
        "\n",
        "1. **text_a** is the text we want to classify, which in this case, is the Data Column field in our Dataframe.\n",
        "2. **text_b** is used if we're training a model to understand the relationship between sentences (i.e. is **text_b** a translation of **text_a**? Is **text_b** an answer to the question asked by **text_a**?). This doesn't apply to our task of Sentiment Analysis, so we can leave **text_b** blank.\n",
        "3. For our task,**label **is the sentiment label of the reviews in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AnNZFHTWBvq"
      },
      "source": [
        "\n",
        "train_InputExamples = train.apply(lambda x: bert.run_classifier.InputExample(guid=None,\n",
        "                                                                   text_a = x[DATA_COLUMN], \n",
        "                                                                   text_b = None, \n",
        "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n",
        "\n",
        "val_InputExamples = val.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n",
        "                                                                   text_a = x[DATA_COLUMN], \n",
        "                                                                   text_b = None, \n",
        "                                                                   label = x[LABEL_COLUMN]), axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rv58n9YpWFEy"
      },
      "source": [
        "\n",
        "train_InputExamples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9aJPy0hbXXH7"
      },
      "source": [
        "print(\"Row 0 - guid of training set : \", train_InputExamples.iloc[0].guid)\n",
        "print(\"\\n__________\\nRow 0 - text_a of training set : \", train_InputExamples.iloc[0].text_a)\n",
        "print(\"\\n__________\\nRow 0 - text_b of training set : \", train_InputExamples.iloc[0].text_b)\n",
        "print(\"\\n__________\\nRow 0 - label of training set : \", train_InputExamples.iloc[0].label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiu0gUSTHSQ3"
      },
      "source": [
        "# Tokenizing the preprocessed text so that it fits the BERT data\n",
        "We do the following in our case :\n",
        "1. Tokenize it \n",
        "2. Break words into WordPieces \n",
        "3. Map the words to indexes using a vocab file that BERT provides(from  BERT tf hub module)\n",
        "4. Add special \"CLS\" and \"SEP\" tokens\n",
        "5. Individual inputs are assigned with \"index\" and \"segment\" tokens.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivsvuZU5WURM"
      },
      "source": [
        "# This is a path to an uncased (all lowercase) version of BERT\n",
        "BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
        "\n",
        "def create_tokenizer_from_hub_module():\n",
        "  \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
        "  with tf.Graph().as_default():\n",
        "    bert_module = hub.Module(BERT_MODEL_HUB)\n",
        "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
        "    with tf.Session() as sess:\n",
        "      vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
        "                                            tokenization_info[\"do_lower_case\"]])\n",
        "      \n",
        "  return bert.tokenization.FullTokenizer(\n",
        "      vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
        "\n",
        "tokenizer = create_tokenizer_from_hub_module()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlKYKHUVX-HP"
      },
      "source": [
        "print(tokenizer.tokenize(train_InputExamples.iloc[0].text_a))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaiN80PpWZt-"
      },
      "source": [
        "\n",
        "# We'll set sequences to be at most 128 tokens long.\n",
        "MAX_SEQ_LENGTH = 128\n",
        "\n",
        "# Convert our train and validation features to InputFeatures that BERT understands.\n",
        "train_features = bert.run_classifier.convert_examples_to_features(train_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "\n",
        "val_features = bert.run_classifier.convert_examples_to_features(val_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sk8GorXIWpV5"
      },
      "source": [
        "print(\"Sentence : \", train_InputExamples.iloc[0].text_a)\n",
        "print(\"-\"*30)\n",
        "print(\"Tokens : \", tokenizer.tokenize(train_InputExamples.iloc[0].text_a))\n",
        "print(\"-\"*30)\n",
        "print(\"Input IDs : \", train_features[0].input_ids)\n",
        "print(\"-\"*30)\n",
        "print(\"Input Masks : \", train_features[0].input_mask)\n",
        "print(\"-\"*30)\n",
        "print(\"Segment IDs : \", train_features[0].segment_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmskal1xJ9Yt"
      },
      "source": [
        "# Fine tuning the model \n",
        "\n",
        "The function create_model is used the set up the model.\n",
        "1. It loads the BERT tf hub module again to extract the computational graph.\n",
        "2. We create a fully-connected layer on top of it to classify the sentiments labels.\n",
        "3. This last layer is trained to adapt the BERT to the sentiment analysis task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHiX-q5JZpxg"
      },
      "source": [
        "def create_model(is_predicting, input_ids, input_mask, segment_ids, labels,\n",
        "                 num_labels):\n",
        "  \n",
        "  bert_module = hub.Module(\n",
        "      BERT_MODEL_HUB,\n",
        "      trainable=True)\n",
        "  bert_inputs = dict(\n",
        "      input_ids=input_ids,\n",
        "      input_mask=input_mask,\n",
        "      segment_ids=segment_ids)\n",
        "  bert_outputs = bert_module(\n",
        "      inputs=bert_inputs,\n",
        "      signature=\"tokens\",\n",
        "      as_dict=True)\n",
        "\n",
        "  # Use \"pooled_output\" for classification tasks on an entire sentence.\n",
        "  # Use \"sequence_outputs\" for token-level output.\n",
        "  output_layer = bert_outputs[\"pooled_output\"]\n",
        "  print(output_layer.shape)\n",
        "\n",
        "  hidden_size = output_layer.shape[-1].value\n",
        "\n",
        "  # Create our own layer to tune for politeness data.\n",
        "  output_weights = tf.get_variable(\n",
        "      \"output_weights\", [num_labels, hidden_size],\n",
        "      initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
        "\n",
        "  output_bias = tf.get_variable(\n",
        "      \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n",
        "\n",
        "  with tf.variable_scope(\"loss\"):\n",
        "\n",
        "    # Dropout helps prevent overfitting\n",
        "    output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
        "\n",
        "    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
        "    logits = tf.nn.bias_add(logits, output_bias)\n",
        "    log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
        "\n",
        "    # Convert labels into one-hot encoding\n",
        "    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
        "    predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n",
        "    # If we're predicting, we want predicted labels and the probabiltiies.\n",
        "    if is_predicting:\n",
        "      return (predicted_labels, log_probs)\n",
        "\n",
        "    # If we're train/eval, compute loss between predicted and actual label\n",
        "    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
        "    loss = tf.reduce_mean(per_example_loss)\n",
        "    return (loss, predicted_labels, log_probs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GP2IYMa5Z3I_"
      },
      "source": [
        "def model_fn_builder(num_labels, learning_rate, num_train_steps,\n",
        "                     num_warmup_steps):\n",
        "  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
        "  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
        "    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
        "\n",
        "    input_ids = features[\"input_ids\"]\n",
        "    input_mask = features[\"input_mask\"]\n",
        "    segment_ids = features[\"segment_ids\"]\n",
        "    label_ids = features[\"label_ids\"]\n",
        "\n",
        "    is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)\n",
        "    \n",
        "    # TRAIN and EVAL\n",
        "    if not is_predicting:\n",
        "\n",
        "      (loss, predicted_labels, log_probs) = create_model(\n",
        "        is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
        "\n",
        "      train_op = bert.optimization.create_optimizer(\n",
        "          loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu=False)\n",
        "      def metric_fn(label_ids, predicted_labels):\n",
        "        accuracy = tf.metrics.accuracy(label_ids, predicted_labels)\n",
        "        true_pos = tf.metrics.true_positives(\n",
        "            label_ids,\n",
        "            predicted_labels)\n",
        "        true_neg = tf.metrics.true_negatives(\n",
        "            label_ids,\n",
        "            predicted_labels)   \n",
        "        false_pos = tf.metrics.false_positives(\n",
        "            label_ids,\n",
        "            predicted_labels)  \n",
        "        false_neg = tf.metrics.false_negatives(\n",
        "            label_ids,\n",
        "            predicted_labels)\n",
        "        \n",
        "        return {\n",
        "            \"eval_accuracy\": accuracy,\n",
        "            \"true_positives\": true_pos,\n",
        "            \"true_negatives\": true_neg,\n",
        "            \"false_positives\": false_pos,\n",
        "            \"false_negatives\": false_neg\n",
        "            }\n",
        "\n",
        "      eval_metrics = metric_fn(label_ids, predicted_labels)\n",
        "\n",
        "      if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "        return tf.estimator.EstimatorSpec(mode=mode,\n",
        "          loss=loss,\n",
        "          train_op=train_op)\n",
        "      else:\n",
        "          return tf.estimator.EstimatorSpec(mode=mode,\n",
        "            loss=loss,\n",
        "            eval_metric_ops=eval_metrics)\n",
        "    else:\n",
        "      (predicted_labels, log_probs) = create_model(\n",
        "        is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
        "      predictions = {\n",
        "          'probabilities': log_probs,\n",
        "          'labels': predicted_labels\n",
        "      }\n",
        "      return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
        "\n",
        "  # Return the actual model function in the closure\n",
        "  return model_fn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjUFizJva94X"
      },
      "source": [
        "#OUTPUT_DIR = '/drive/MyDrive/DataMining_NLP_original'\n",
        "import os\n",
        "\n",
        "path = '/content/gdrive/MyDrive/Data Science and AI/Research in Data Mining/Training_NLP' + prepr\n",
        "#path = '/drive/MyDrive/DataMining_NLP_rmv_punct'\n",
        "OUTPUT_DIR = path\n",
        "#OUTPUT_DIR = '/drive/MyDrive/DataMining_NLP'\n",
        "#OUTPUT_DIR = '/drive/MyDrive/DataMining_NLP2'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXcFb6l1aEZt"
      },
      "source": [
        "# Compute train and warmup steps from batch size\n",
        "# These hyperparameters are copied from this colab notebook (https://colab.sandbox.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb)\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 2e-5\n",
        "NUM_TRAIN_EPOCHS = 3.0\n",
        "# Warmup is a period of time where the learning rate is small and gradually increases--usually helps training.\n",
        "WARMUP_PROPORTION = 0.1\n",
        "# Model configs\n",
        "SAVE_CHECKPOINTS_STEPS = 300\n",
        "SAVE_SUMMARY_STEPS = 100\n",
        "\n",
        "# Compute train and warmup steps from batch size\n",
        "num_train_steps = int(len(train_features) / BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
        "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n",
        "\n",
        "run_config = tf.estimator.RunConfig(\n",
        "    model_dir=OUTPUT_DIR,\n",
        "    save_summary_steps=SAVE_SUMMARY_STEPS,\n",
        "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)\n",
        "\n",
        "# Specify output directory and number of checkpoint steps to save\n",
        "run_config = tf.estimator.RunConfig(\n",
        "    model_dir=OUTPUT_DIR,\n",
        "    save_summary_steps=SAVE_SUMMARY_STEPS,\n",
        "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTc7OPJ8JUTP"
      },
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-JN7ai5aTPl"
      },
      "source": [
        "model_fn = model_fn_builder(\n",
        "  num_labels=len(label_list),\n",
        "  learning_rate=LEARNING_RATE,\n",
        "  num_train_steps=num_train_steps,\n",
        "  num_warmup_steps=num_warmup_steps)\n",
        "\n",
        "estimator = tf.estimator.Estimator(\n",
        "  model_fn=model_fn,\n",
        "  config=run_config,\n",
        "  params={\"batch_size\": BATCH_SIZE})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMMdgKlzajNB"
      },
      "source": [
        "\n",
        "# Create an input function for training. drop_remainder = True for using TPUs.\n",
        "train_input_fn = bert.run_classifier.input_fn_builder(\n",
        "    features=train_features,\n",
        "    seq_length=MAX_SEQ_LENGTH,\n",
        "    is_training=True,\n",
        "    drop_remainder=False)\n",
        "\n",
        "# Create an input function for validating. drop_remainder = True for using TPUs.\n",
        "val_input_fn = run_classifier.input_fn_builder(\n",
        "    features=val_features,\n",
        "    seq_length=MAX_SEQ_LENGTH,\n",
        "    is_training=False,\n",
        "    drop_remainder=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Z46c69uamZ3"
      },
      "source": [
        "#Training the model\n",
        "print(f'Beginning Training!')\n",
        "current_time = datetime.now()\n",
        "training_output = estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
        "print(\"Training took time \", datetime.now() - current_time)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxP6afGNE-b3"
      },
      "source": [
        "print(training_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkgrNybbao0y"
      },
      "source": [
        "output = estimator.evaluate(input_fn=val_input_fn, steps=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCqa4eOGicTN"
      },
      "source": [
        "!pip install --upgrade gspread"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hy4QmtVKtGc"
      },
      "source": [
        "# Output\n",
        "Creates a google spread sheet to store the results of the individual preprocessing tasks. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9qqhW-NfpsL"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "import gspread\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "gc = gspread.authorize(GoogleCredentials.get_application_default())\n",
        "\n",
        "# Open our new sheet and add some data.\n",
        "spreadsheet = gc.open('Bert results')\n",
        "\n",
        "values = [prepr]\n",
        "for h in output:\n",
        "  values.append(str(output[h]))\n",
        "\n",
        "#worksheet.update('A1:G2',output)\n",
        "spreadsheet.values_append('Sheet1', {'valueInputOption': 'USER_ENTERED'}, {'values': [values]})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6H52bQdaILL4"
      },
      "source": [
        "%rm -rf '/content/gdrive'"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}